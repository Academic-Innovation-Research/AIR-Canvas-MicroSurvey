#!/usr/bin/env python3
"""
Generate SQL INSERT statements for Enrollment table
from Canvas roster CSVs and mapping in Notes.md.

CSV expected structure (column positions):
0 Profile Picture | 1 Name | 2 Login ID | 3 SIS ID | 4 Section | 5 Role | ...

Notes.md structure:
<file_prefix>
https://erau.instructure.com/courses/<CanvasID>/

Blank line between entries.

Usage: 
cd /Users/robert/Downloads/Canvas

# 1. Run the Python script to generate the SQL file
python3 build_enrollment_inserts.py --root . --outfile enrollment_inserts.sql

# 2. Then open the generated file in your default text editor
open enrollment_inserts.sql


"""

import csv
import sys
from pathlib import Path
import argparse
from typing import Dict, List, Tuple

# CSV column positions (0-based)
IDX_EMPL = 3
IDX_ROLE = 5

def escape_mysql_literal(val: str) -> str:
    if val is None:
        return "NULL"
    s = val.replace("\u00A0", " ").replace("\r\n", "\n").replace("\r", "\n").strip()
    s = s.replace("\n", " ").replace("\t", " ")
    s = s.replace("\\", "\\\\").replace("'", "\\'")
    return f"'{s}'"

def parse_notes_md(notes_path: Path) -> Dict[str, int]:
    mapping = {}
    if not notes_path.exists():
        print(f"✖ Notes.md not found at {notes_path}", file=sys.stderr)
        return mapping
    with notes_path.open("r", encoding="utf-8") as f:
        lines = [l.strip() for l in f if l.strip()]
    # Pair lines: prefix, URL
    for i in range(0, len(lines), 2):
        prefix = lines[i]
        url = lines[i+1] if i+1 < len(lines) else ""
        try:
            canvas_id = int(url.rstrip("/").split("/")[-1])
            mapping[prefix] = canvas_id
        except ValueError:
            print(f"⚠ Skipping mapping for {prefix}, invalid URL: {url}", file=sys.stderr)
    return mapping

def read_csv_rows(path: Path) -> List[Tuple[str, str]]:
    """Return list of (Empl_ID, Role) from CSV."""
    rows = []
    try:
        with path.open("r", encoding="utf-8-sig", newline="") as f:
            reader = csv.reader(f)
            first = True
            for cols in reader:
                if first:
                    first = False
                    continue
                if len(cols) <= max(IDX_EMPL, IDX_ROLE):
                    continue
                empl = cols[IDX_EMPL].strip()
                role = cols[IDX_ROLE].strip()
                if empl and role:
                    rows.append((empl, role))
    except Exception as e:
        print(f"✖ Error reading {path}: {e}", file=sys.stderr)
    return rows

def batch_insert_values(canvas_id: int, rows: List[Tuple[str, str]], batch_size: int = 500) -> List[str]:
    stmts = []
    for i in range(0, len(rows), batch_size):
        chunk = rows[i:i+batch_size]
        values_sql = []
        for empl, role in chunk:
            values_sql.append(f"({canvas_id}, {escape_mysql_literal(empl)}, {escape_mysql_literal(role)})")
        stmt = (
            "INSERT INTO `Enrollment` (CanvasID, Empl_ID, Role)\nVALUES\n  "
            + ",\n  ".join(values_sql)
            + "\nON DUPLICATE KEY UPDATE Empl_ID = Empl_ID;"
        )
        stmts.append(stmt)
    return stmts

def main():
    ap = argparse.ArgumentParser(description="Generate Enrollment INSERT SQL from CSVs + Notes.md mapping.")
    ap.add_argument("--root", default=".", help="Directory to scan recursively for CSV and Notes.md")
    ap.add_argument("--outfile", default="enrollment_inserts.sql", help="Output .sql file")
    ap.add_argument("--stdout", action="store_true", help="Print SQL to stdout instead of writing file")
    ap.add_argument("--dbname", default="Micro-Surveys", help="DB name for USE `<db>`;")
    ap.add_argument("--no-use", action="store_true", help="Do not emit USE `<db>`;")
    ap.add_argument("--batch", type=int, default=500, help="Rows per INSERT batch")
    args = ap.parse_args()

    root = Path(args.root).expanduser().resolve()
    notes_path = root / "Notes.md"

    mapping = parse_notes_md(notes_path)
    if not mapping:
        print("No mappings found in Notes.md.", file=sys.stderr)
        return 1

    csv_files = sorted({p for pat in ("*.csv", "*.CSV") for p in root.rglob(pat) if p.is_file()})
    if not csv_files:
        print("No CSV files found.", file=sys.stderr)
        return 1

    sql_lines = []
    sql_lines.append("-- Generated by build_enrollment_inserts.py")
    sql_lines.append("SET NAMES utf8mb4;")
    if not args.no_use and args.dbname:
        sql_lines.append(f"USE `{args.dbname}`;")
    sql_lines.append("")

    for csv_path in csv_files:
        prefix = csv_path.stem
        canvas_id = mapping.get(prefix)
        if not canvas_id:
            print(f"⚠ No CanvasID mapping for {prefix}, skipping.", file=sys.stderr)
            continue
        rows = read_csv_rows(csv_path)
        if not rows:
            print(f"⚠ No valid rows in {csv_path}", file=sys.stderr)
            continue
        sql_lines.extend(batch_insert_values(canvas_id, rows, batch_size=args.batch))
        sql_lines.append("")

    sql_text = "\n".join(sql_lines)
    if args.stdout:
        print(sql_text)
    else:
        outpath = Path(args.outfile).expanduser().resolve()
        outpath.parent.mkdir(parents=True, exist_ok=True)
        with outpath.open("w", encoding="utf-8") as f:
            f.write(sql_text)
        print(f"✅ Wrote SQL to {outpath}")

    return 0

if __name__ == "__main__":
    sys.exit(main())
